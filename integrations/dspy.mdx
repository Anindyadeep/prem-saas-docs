---

title: "DsPY"

description: "This example explains how to use DsPY by Stanford NLP to interact with different chat models."

---

### Stanford DsPY 

[DSPy by Stanford NLP](https://github.com/stanfordnlp/dspy) is a framework designed to optimize language model (LM) prompts and weights, simplifying the process of building complex systems with LMs by automating and unifying techniques for prompting, fine-tuning, and reasoning. It provides composable and declarative modules in Pythonic syntax and includes an automatic compiler to instruct LMs on executing declarative steps. Qdrant can be integrated as a retrieval mechanism within the DSPy flow.

### Installation and Setup

We start by installing `dspy` and `premai-sdk`. Use the following commands to install them:

```bash
pip install dspy premai
```

Before proceeding, please ensure that you have created an account on PremAI and started a project. If not, refer to the [quick start](https://docs.premai.io/introduction) guide to get started with the PremAI platform, create your first project, and obtain your API key.

```python
from dspy import PremAI
```

### Setup PremAI instance with DsPY

Once we have imported our required modules, let's set up our dspy-premai client. For now, let's assume that our `project_id` is 123. However, be sure to use your actual project ID; otherwise, it will throw an error.

To use dspy with PremAI, you do not need to pass any `model` parameter or model name. It will use the default model and parameters deployed from the [LaunchPad](https://docs.premai.io/get-started/launchpad).

<Note>
**Note**: If you change the `model` or any other parameter like `temperature` while setting up the client, it will override the existing default configurations. You can do this during experimentation, but it is not recommended for production use.
</Note>

```python
import os
import getpass

if "PREMAI_API_KEY" not in os.environ:
    os.environ["PREMAI_API_KEY"] = getpass.getpass("PremAI API Key:")
```

### Calling the model

Here is a quick example of how to get a response from the model:

```python
llm = PremAI(project_id=123, api_key="your-premai-api-key")
print(llm("What is a large language model?"))
```

You can also change the generation parameters while calling the model. Here's how you can do that:

```python
print(llm(
    "What is a large language model?", 
    temperature=0.7, max_tokens=20
))
```

### Important Notes

Before proceeding further, please note that the current version of premai-sdk does not support the parameters [n](https://platform.openai.com/docs/api-reference/chat/create#chat-create-n) and [stop](https://platform.openai.com/docs/api-reference/chat/create#chat-create-stop). Support for these parameters will be provided in future versions.

