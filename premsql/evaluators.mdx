---
title: PremSQL Evaluators
description: Connects to databases and executes generated SQL queries to fetch results and evaluates them.
---

## Evaluators and Executors

premsql evaluators consist of two things. It comes with executors which helps to connect and execute SQL to the DB. PremSQL Text2SQLEvaluator lets you to evaluate your models / pipelines using the executors w.r.t. a gold dataset. Our evaluators supports the following metrics as of now: 

1. **Execution Accuracy (EX)**
2. **Valid Efficiency Score (VES)**

### Execution Accuracy (EX)
Execution Accuracy measures the correctness of the SQL generated by the model by comparing the executed results with the ground truth SQL. This metric gives a direct measure of how accurate the generated SQL is.

### Valid Efficiency Score (VES)
The primary objective of the SQL queries generated by Large Language Models (LLMs) is accuracy. However, performance is also crucial when dealing with large datasets. The Valid Efficiency Score assesses both accuracy and performance, ensuring that the generated query is optimized for execution. The figure below illustrates how this metric is computed:


## Evaluation Setup

Let's dive into the code to see how you can use `premsql` to evaluate models or pipelines using these metrics.

### Step 1: Import Required Libraries

First, import the necessary libraries to set up the evaluation environment.

```python
import json
from pathlib import Path 
from premsql.evaluator import Text2SQLEvaluator, SQLiteExecutor
```

### Step 2: Prepare the Experiment Path

Our evaluation methods are model and pipeline agnostic. To evaluate, we rely on a special response JSON structure. Ensure that you have saved all the model responses inside a JSON file. Below, we define the `experiment_path`. You can manually specify this path or obtain it from your generator object.

```python
experiment_path = Path(
    "experiments/test/testing_finetuned_deepseek_full_fewshot/"
)
responses = json.load(open(experiment_path / "predict.json", "r"))
```

### Step 3: Define the Executor

Since text-to-SQL evaluation depends on database execution, an executor object is required. An executor implements the `execute_sql` method from the `premsql.evaluator.base.BaseExecutor` abstract class. Below, we use `SQLiteExecutor` for SQLite databases, but you can create custom executors for other database types.

```python
executor = SQLiteExecutor()
evaluator = Text2SQLEvaluator(
    executor=executor, experiment_path=experiment_path
)
```

### Step 4: Compute Execution Accuracy

With the setup complete, compute the Execution Accuracy score using the `premsql` evaluator.

```python
ex = evaluator.execute(
    metric_name="accuracy", 
    model_responses=responses, 
    filter_by="difficulty"
)
```

You can filter the responses using any criteria, similar to filtering datasets. Letâ€™s print the results to see the scores.

```python
print(f" Execution Accuracy is: {ex}")
```

### Step 5: Evaluating and Saving Results

Upon completion, results are saved in the `experiment_path`, providing detailed information about each validation question, such as errors and debugging insights.

```json
{
    "question_id": 23,
    "db_id": "california_schools",
    "question": "List the names of schools with more than 30 difference in enrollments between K-12 and ages 5-17? Please also give the full street address of the schools.",
    "evidence": "Difference in enrollment = `Enrollment (K-12)` - `Enrollment (Ages 5-17)`",
    "SQL": "SELECT T1.School, T1.Street FROM schools AS T1 INNER JOIN frpm AS T2 ON T1.CDSCode = T2.CDSCode WHERE T2.`Enrollment (K-12)` - T2.`Enrollment (Ages 5-17)` > 30",
    "difficulty": "moderate",
    "db_path": "data/bird/validation/dev_databases/california_schools/california_schools.sqlite",
    "prompt": "<begin of sentence>You are an ...",
    "dataset_type": "real",
    "generated": "SELECT T2.`School Name`, T2.Street, T2.City, T2.Zip FROM satscores AS T1 INNER JOIN frpm AS T2 ON T1.cds = T2.CDSCode WHERE T1.`Enrollment (K-12)` - T1.`Enrollment (Ages 5-17)` > 30",
    "error": "no such column: T2.Street",
    "accuracy": 0
}
```

### Step 6: Compute Valid Efficiency Score (VES)

To compute the VES, use the same method, but with different filters. For instance, here, we filter results based on `db_id`.

```python
ves = evaluator.execute(
    metric_name="ves", 
    model_responses=responses, 
    filter_by="db_id"
)
```

### Step 7: Plotting VES Results

Visualize the results to understand performance across different databases. Such analysis helps identify areas for improvement.

```python
import matplotlib.pyplot as plt

ves.pop("overall")

plt.figure(figsize=(12, 6))
plt.bar(ves.keys(), ves.values(), color='skyblue')
plt.xticks(rotation=45, ha='right')
plt.ylabel('Values')
plt.title('Values of Different Categories Excluding Overall')
plt.grid(axis='y', linestyle='--', alpha=0.6)
plt.tight_layout()
plt.show()
```

## Writing Your Custom Executor

`premsql` evaluators are plug-and-play tools that can be adapted for any database type. Below is an example of writing a custom executor for PostgreSQL.

### Example: PostgreSQL Executor

Simply inherit the `BaseExecutor` class and define the `execute_sql` function.

```python
import psycopg2
import time
from premsql.evaluator.base import BaseExecutor

class PostgreSQLExecutor(BaseExecutor):
    def execute_sql(self, sql: str, dsn_or_db_path: str) -> dict:
        conn = psycopg2.connect(dsn_or_db_path)
        cursor = conn.cursor()

        start_time = time.time()
        try:
            cursor.execute(sql)
            result = cursor.fetchall()
            error = None
        except Exception as e:
            result = None
            error = str(e)

        end_time = time.time()
        cursor.close()
        conn.close()

        result = {
            "result": result,
            "error": error,
            "execution_time": end_time - start_time,
        }
        return result
```
